import { NextRequest, NextResponse } from 'next/server'
import { getAuthUser } from '@/lib/middleware'
import pool from '@/lib/db'
import { GoogleGenAI } from '@google/genai'

// Function to analyze image and get detailed character description using Gemini
async function analyzeImageForCharacter(imageData: string, mimeType: string): Promise<string> {
  const geminiApiKey = process.env.GEMINI_API_KEY || process.env.VEO_API_KEY
  if (!geminiApiKey) {
    throw new Error('Gemini API key not configured')
  }

  const geminiClient = new GoogleGenAI({ apiKey: geminiApiKey })
  
  const analysisPrompt = `You are an expert character artist. Analyze this image with MICROSCOPIC DETAIL to ensure PERFECT visual replication across multiple video clips. Every feature must be described so precisely that the character will look IDENTICAL in every frame.

ðŸŽ¯ CRITICAL MISSION: Create a description so detailed that NO variation is possible. Think of this as creating a DNA blueprint for visual consistency.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“¸ FACIAL FEATURES (EXTREME PRECISION REQUIRED)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FACE STRUCTURE:
â€¢ Overall face shape (oval/round/square/heart/diamond/oblong - be extremely specific)
â€¢ Face width-to-height ratio
â€¢ Facial symmetry or asymmetry details
â€¢ Bone structure prominence (high/low cheekbones, defined/soft jawline)
â€¢ Face fullness (gaunt/normal/full/very full cheeks)

SKIN:
â€¢ Exact skin tone with specific color descriptors (pale ivory/warm beige/golden tan/deep brown/etc.)
â€¢ Skin texture (smooth/textured/pores visible/matte/dewy)
â€¢ Any freckles, moles, birthmarks (exact locations and sizes)
â€¢ Skin undertones (cool/warm/neutral)
â€¢ Any scars, wrinkles, or distinguishing marks

EYES (ULTRA-DETAILED):
â€¢ Eye shape (almond/round/hooded/deep-set/upturned/downturned)
â€¢ Exact eye color with specifics (not just "brown" but "dark chocolate brown with amber flecks")
â€¢ Eye size relative to face
â€¢ Distance between eyes (close-set/normal/wide-set)
â€¢ Upper and lower eyelid characteristics
â€¢ Eyelash length, thickness, and curl
â€¢ Eyebrow-to-eye distance
â€¢ Whites of eyes visibility
â€¢ Pupil size and iris patterns if visible

EYEBROWS:
â€¢ Exact shape (straight/arched/soft arch/angular/s-shaped)
â€¢ Thickness (thin/medium/thick/bushy)
â€¢ Color (include if different from hair)
â€¢ Arch height and position
â€¢ Starting point, peak position, and tail end
â€¢ Hair density and direction of growth
â€¢ Gap between eyebrows

NOSE:
â€¢ Overall nose shape (straight/button/Roman/hawk/snub/upturned/etc.)
â€¢ Bridge width (narrow/medium/wide)
â€¢ Bridge height (high/low/flat)
â€¢ Nostril shape and size
â€¢ Nose tip shape (pointed/round/bulbous/flat)
â€¢ Nose length relative to face
â€¢ Nose angle from profile
â€¢ Alar base width

MOUTH AND LIPS:
â€¢ Upper lip shape and fullness (thin/medium/full/very full)
â€¢ Lower lip shape and fullness
â€¢ Lip color (pale pink/rosy/mauve/deep red/brown-toned/etc.)
â€¢ Cupid's bow definition (sharp/soft/flat)
â€¢ Mouth width relative to nose
â€¢ Lip-to-nose distance
â€¢ Smile characteristics (if visible)
â€¢ Teeth visibility and characteristics
â€¢ Lip texture (smooth/lined/glossy/matte)

JAWLINE AND CHIN:
â€¢ Jaw shape (angular/soft/square/rounded/v-shaped)
â€¢ Jaw width
â€¢ Jawline definition (sharp/soft/undefined)
â€¢ Chin shape (pointed/square/round/cleft/receding/prominent)
â€¢ Chin size relative to face
â€¢ Jowl presence or absence

CHEEKBONES AND CHEEKS:
â€¢ Cheekbone prominence (high/low/flat)
â€¢ Cheekbone width
â€¢ Cheek fullness (hollow/normal/full/very full)
â€¢ Apple of cheeks visibility

EARS (if visible):
â€¢ Size relative to head
â€¢ Shape and position
â€¢ Lobe type (attached/detached)
â€¢ Any piercings or jewelry

AGE APPEARANCE:
â€¢ Estimated age range (be specific: early 20s, mid-30s, late 40s, etc.)
â€¢ Signs of aging if any (crow's feet, forehead lines, laugh lines, neck lines)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ’‡ HAIR (COMPLETE SPECIFICATION)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â€¢ Exact color with undertones (jet black/dark brown with red undertones/honey blonde with caramel highlights/salt and pepper/etc.)
â€¢ Hair length (specific: shoulder-length, chin-length, waist-length, buzz cut 3mm, etc.)
â€¢ Hair texture (straight/wavy/curly/kinky - specify curl pattern if curly: 2A, 3B, 4C, etc.)
â€¢ Hair thickness/volume (thin/medium/thick/very thick)
â€¢ Hair part location and width (center/left/right/no part)
â€¢ Hairline shape (straight/widow's peak/receding/rounded)
â€¢ Styling (loose/sleek/messy/tied back/specific style name)
â€¢ Hair shine level (matte/natural/glossy)
â€¢ Flyaways or baby hairs if present
â€¢ Facial hair if any (mustache/beard style, length, color, density)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ‘” CLOTHING & ACCESSORIES (EXACT REPLICATION)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CLOTHING:
â€¢ Every garment type (shirt/sweater/jacket/dress/etc.)
â€¢ Exact colors with specific names (not "blue" but "navy blue" or "sky blue")
â€¢ Patterns or prints (solid/striped/plaid/floral - describe pattern details)
â€¢ Fabric texture appearance (cotton/silk/knit/denim/leather/etc.)
â€¢ Neckline style (crew/v-neck/collar/turtleneck/scoop/etc.)
â€¢ Sleeve length and style
â€¢ Fit (tight/fitted/loose/oversized)
â€¢ Any logos, text, or graphics on clothing
â€¢ Layering details
â€¢ Visible buttons, zippers, pockets

ACCESSORIES:
â€¢ Jewelry (earrings/necklace/rings/bracelet/watch - describe each piece)
â€¢ Glasses (frame shape, color, thickness if wearing)
â€¢ Hat or headwear (style, color, material)
â€¢ Scarf or tie (color, pattern, how worn)
â€¢ Any other accessories (bag visible, pins, badges, etc.)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸŽ¬ ENVIRONMENT & SETTING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BACKGROUND:
â€¢ Primary background elements (wall/room/outdoor/studio/etc.)
â€¢ Background colors (exact shades)
â€¢ Background texture (smooth/textured/patterned)
â€¢ Props or furniture visible (describe each)
â€¢ Background blur level (sharp/slightly blurred/very blurred)

LIGHTING:
â€¢ Light direction (front/side/back/above)
â€¢ Light quality (soft/hard/diffused/dramatic)
â€¢ Light color temperature (warm/neutral/cool)
â€¢ Shadow characteristics
â€¢ Highlight placement on face

CAMERA/FRAMING:
â€¢ Shot type (close-up/medium/wide)
â€¢ Camera angle (eye-level/slightly above/slightly below)
â€¢ Character position in frame (centered/left/right)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“‹ OUTPUT FORMAT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Start with: "EXACT CHARACTER SPECIFICATION FOR VIDEO CONSISTENCY:"

Then provide the complete analysis in a structured format. Be EXHAUSTIVELY detailed. Use precise measurements and comparisons. Every detail matters for perfect replication across clips.`

  try {
    const response = await geminiClient.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{
        role: 'user',
        parts: [
          { text: analysisPrompt },
          { 
            inlineData: {
              mimeType: mimeType,
              data: imageData
            }
          }
        ]
      }]
    })

    const description = response.text || ''
    console.log('Character analysis result:', description.substring(0, 200))
    return description
  } catch (error) {
    console.error('Error analyzing image:', error)
    return '' // Return empty string if analysis fails, video gen will still work
  }
}

// Veo 3.1 video generation using Google Gen AI SDK
export async function POST(request: NextRequest) {
  try {
    // Check authentication
    const user = await getAuthUser(request)
    if (!user) {
      return NextResponse.json(
        { success: false, error: 'Unauthorized' },
        { status: 401 }
      )
    }

    const body = await request.json()
    const { 
      prompt, 
      scriptSections, // Array of clip descriptions for multi-clip generation
      videoStyle = 'cinematic', // 'dialogue', 'cinematic', 'animation'
      aspectRatio = '16:9',
      duration = 8, // Total duration (8, 16, 24, or 32 seconds)
      sourceImage, // Base64 image for image-to-video mode
      inputType = 'text-to-video', // 'image-to-video' or 'text-to-video'
    } = body

    // Determine clips to generate
    const clips = scriptSections && scriptSections.length > 0 
      ? scriptSections 
      : [prompt]
    
    const clipCount = clips.length

    // For image-to-video, a prompt is optional (we'll use the character description)
    // For text-to-video, a prompt is required
    if (inputType === 'text-to-video' && (!clips[0] || !clips[0].trim())) {
      return NextResponse.json(
        { success: false, error: 'Prompt is required for text-to-video' },
        { status: 400 }
      )
    }

    // For image-to-video without a prompt, use a default
    if (inputType === 'image-to-video' && (!clips[0] || !clips[0].trim())) {
      clips[0] = 'Animate this image naturally with subtle movements and bring it to life'
    }

    // Calculate credit cost (15 credits per second for Veo 3.1)
    // For single clip: use full duration, for multiple clips: each is 8 seconds
    const durationPerClip = clipCount === 1 ? duration : 8
    const creditCost = 15 * clipCount * durationPerClip

    // Check user credits
    const creditsResult = await pool.query(
      'SELECT remaining_credits FROM credits WHERE user_id = $1',
      [user.id]
    )
    
    const userCredits = creditsResult.rows[0]?.remaining_credits || 0
    if (userCredits < creditCost) {
      return NextResponse.json(
        { success: false, error: `Insufficient credits. Need ${creditCost}, have ${userCredits}` },
        { status: 400 }
      )
    }

    const apiKey = process.env.VEO_API_KEY
    if (!apiKey) {
      return NextResponse.json(
        { success: false, error: 'Veo API key not configured' },
        { status: 500 }
      )
    }

    // Initialize Google Gen AI client with Veo API key
    const client = new GoogleGenAI({ apiKey })

    // If image-to-video mode, analyze the image FIRST to get character description
    let characterDescription = ''
    let imageData = ''
    let imageMimeType = 'image/png'
    
    if (inputType === 'image-to-video' && sourceImage) {
      // Extract base64 data from data URL if present
      imageData = sourceImage
      
      if (sourceImage.startsWith('data:')) {
        const matches = sourceImage.match(/^data:([^;]+);base64,(.+)$/)
        if (matches) {
          imageMimeType = matches[1]
          imageData = matches[2]
        }
      }
      
      // Analyze image to get detailed character description (happens automatically)
      console.log('Analyzing image for character description...')
      characterDescription = await analyzeImageForCharacter(imageData, imageMimeType)
      console.log('Character description obtained:', characterDescription ? 'Yes' : 'No')
    }

    // Generate videos for each clip
    const operationNames: string[] = []
    
    for (let i = 0; i < clips.length; i++) {
      const clipPrompt = clips[i]
      
      // Use the prompt exactly as provided by the user - no style modifications
      const enhancedPrompt = clipPrompt

      console.log(`Starting Veo 3.1 Fast generation for clip ${i + 1}/${clipCount}:`, enhancedPrompt.substring(0, 100))
      console.log(`Input type: ${inputType}, Has source image: ${!!sourceImage}, Duration: ${durationPerClip}s`)

      // Build generation config
      const generateConfig: any = {
        aspectRatio: aspectRatio,
        numberOfVideos: 1,
        durationSeconds: durationPerClip, // Use configurable duration (8, 16, 24, or 32 seconds for single clip)
      }

      // Build request options based on input type
      const requestOptions: any = {
        model: 'veo-3.1-fast-generate-preview',
        config: generateConfig,
      }

      // Handle image-to-video mode with character description
      if (inputType === 'image-to-video' && sourceImage && imageData) {
        // SMART STRATEGY BASED ON VIDEO LENGTH:
        // Single clip (8s): Just animate the image naturally, no extra specs
        // Multiple clips (16s+): Use strict character consistency
        
        if (clipCount === 1) {
          // SINGLE CLIP: Simple animation, no modifications (uses selected duration: 8-32s)
          requestOptions.image = {
            imageBytes: imageData,
            mimeType: imageMimeType
          }
          // Use user's prompt as-is, or simple default
          requestOptions.prompt = enhancedPrompt || 'Animate this image naturally with smooth, realistic movements'
          console.log(`Single clip (${durationPerClip}s): Simple animation without character specifications`)
        } else if (i === 0) {
          // FIRST CLIP of MULTI-CLIP VIDEO: Animate image with character description for consistency
          requestOptions.image = {
            imageBytes: imageData,
            mimeType: imageMimeType
          }
          
          // Include character description for multi-clip consistency
          if (characterDescription) {
            requestOptions.prompt = `MAINTAIN THIS EXACT CHARACTER THROUGHOUT: ${characterDescription}

${enhancedPrompt}

CRITICAL: Keep the character's face, features, clothing, and appearance EXACTLY as shown in the reference image. No variations or changes to the character's appearance.`
          } else if (enhancedPrompt) {
            requestOptions.prompt = `${enhancedPrompt}\n\nIMPORTANT: Animate this image naturally while keeping the character's appearance exactly the same throughout.`
          }
          console.log(`Clip ${i + 1}/${clipCount}: First clip with character description for consistency`)
        } else {
          // SUBSEQUENT CLIPS: Use STRICT character description for perfect consistency
          if (characterDescription) {
            const characterPrompt = `EXACT CHARACTER CONTINUATION - VISUAL CONSISTENCY LOCKED:

CHARACTER SPECIFICATIONS (MUST MATCH EXACTLY):
${characterDescription}

CONTINUATION RULES:
âœ“ Face: IDENTICAL facial features, skin tone, expressions
âœ“ Body: SAME body type, posture, proportions
âœ“ Clothing: EXACT same outfit, colors, style, accessories
âœ“ Background: SAME environment, lighting, setting (unless script specifies otherwise)
âœ“ Camera angle: Similar perspective for natural flow
âœ— NO changes to character appearance
âœ— NO different clothing or hairstyle
âœ— NO different person or face

SCENE DESCRIPTION FOR THIS CLIP:
${enhancedPrompt}

NOTE: This is a continuation. Only the character's ACTION and DIALOGUE change. Their appearance, clothing, and environment stay IDENTICAL to previous clips.`
            requestOptions.prompt = characterPrompt
            console.log(`Clip ${i + 1}/${clipCount}: Using ENHANCED character consistency prompt`)
          } else {
            // Fallback if character analysis failed
            requestOptions.prompt = `CRITICAL CONTINUATION: This is Clip ${i + 1} of a ${clipCount}-clip sequence. The character, clothing, background, and lighting must be VISUALLY IDENTICAL to the previous clip(s). Same face, same outfit, same location. Only the character's action/dialogue changes.

Scene: ${enhancedPrompt}

REMINDER: Character appearance is locked from previous clips.`
            console.log(`Clip ${i + 1}/${clipCount}: Using enhanced fallback continuation prompt`)
          }
        }
      } else {
        // Text-to-video mode (no image)
        requestOptions.prompt = enhancedPrompt
      }

      // Start video generation with Veo 3.1 Fast using SDK
      const operation = await client.models.generateVideos(requestOptions)

      if (!operation.name) {
        return NextResponse.json(
          { success: false, error: `No operation name returned for clip ${i + 1}` },
          { status: 500 }
        )
      }

      console.log(`Clip ${i + 1} operation started:`, operation.name)
      operationNames.push(operation.name)
      
      // Small delay between API calls to avoid rate limiting
      if (i < clips.length - 1) {
        await new Promise(resolve => setTimeout(resolve, 2000))
      }
    }

    // Deduct credits after starting all generations
    await pool.query(
      `UPDATE credits 
       SET remaining_credits = remaining_credits - $1,
           used_credits = used_credits + $1
       WHERE user_id = $2`,
      [creditCost, user.id]
    )

    // Record the transaction
    let actionDescription = ''
    if (inputType === 'image-to-video') {
      actionDescription = `Image to Video - ${videoStyle} style - ${duration}s (${clipCount} clips)`
    } else {
      actionDescription = `Text to Video - ${videoStyle} style - ${duration}s (${clipCount} clips)`
    }
    
    await pool.query(
      `INSERT INTO credit_transactions (user_id, action_type, credits_used, model_used, description)
       VALUES ($1, $2, $3, $4, $5)`,
      [user.id, 'video_generation', creditCost, 'veo-3.1-fast', actionDescription]
    )

    // Get updated credits
    const updatedCredits = await pool.query(
      'SELECT remaining_credits FROM credits WHERE user_id = $1',
      [user.id]
    )

    return NextResponse.json({
      success: true,
      operationNames: operationNames,
      operationName: operationNames[0], // For backwards compatibility
      clipCount: clipCount,
      message: `Video generation started for ${clipCount} clip${clipCount > 1 ? 's' : ''}`,
      videoStyle,
      duration,
      creditsUsed: creditCost,
      remainingCredits: updatedCredits.rows[0]?.remaining_credits || 0
    })

  } catch (error: any) {
    console.error('Gemini video generation error:', error)
    
    // Handle specific error types with user-friendly messages
    const errorMessage = error?.message || error?.toString() || 'Internal server error'
    let userMessage = errorMessage
    let statusCode = 500
    
    // Check for quota/rate limit errors
    if (errorMessage.includes('429') || 
        errorMessage.includes('quota') || 
        errorMessage.includes('RESOURCE_EXHAUSTED') ||
        errorMessage.includes('rate limit')) {
      userMessage = 'API quota exceeded. The daily limit for video generation has been reached. Please try again tomorrow or upgrade your plan at https://ai.google.dev/pricing'
      statusCode = 429
    } else if (errorMessage.includes('401') || errorMessage.includes('UNAUTHENTICATED')) {
      userMessage = 'API authentication failed. Please check your Veo API key configuration.'
      statusCode = 401
    } else if (errorMessage.includes('content') || errorMessage.includes('moderation') || errorMessage.includes('safety')) {
      userMessage = 'Your prompt was blocked by content moderation. Please try a different prompt.'
      statusCode = 400
    }
    
    return NextResponse.json(
      { success: false, error: userMessage },
      { status: statusCode }
    )
  }
}
